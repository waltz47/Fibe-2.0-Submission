{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AzOVab4x4ei",
        "outputId": "898282d6-103b-4c68-c7d0-b5d13c4dba78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-23 13:10:47--  https://uc.hackerearth.com/he-public-data/dataset1c2f4b7.zip\n",
            "Resolving uc.hackerearth.com (uc.hackerearth.com)... 13.35.210.75, 13.35.210.90, 13.35.210.84, ...\n",
            "Connecting to uc.hackerearth.com (uc.hackerearth.com)|13.35.210.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 621018124 (592M) [binary/octet-stream]\n",
            "Saving to: ‘dataset1c2f4b7.zip’\n",
            "\n",
            "dataset1c2f4b7.zip  100%[===================>] 592.25M   381MB/s    in 1.6s    \n",
            "\n",
            "2024-09-23 13:10:48 (381 MB/s) - ‘dataset1c2f4b7.zip’ saved [621018124/621018124]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://uc.hackerearth.com/he-public-data/dataset1c2f4b7.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TEHzppjMs_E",
        "outputId": "3466edfc-786a-4465-a998-30fb6b2c9e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  dataset1c2f4b7.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/sample_submission.csv  \n",
            "  inflating: dataset/test.csv        \n",
            "  inflating: dataset/train.csv       \n"
          ]
        }
      ],
      "source": [
        "!unzip dataset1c2f4b7.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgE5zSEPNNng"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "df = pd.read_csv('/content/dataset/train.csv',encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biAUMjBTIAJ-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "\n",
        "def clean_text(text):\n",
        "    # Normalize unicode characters\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
        "    text = re.sub(r'•|\\u2022|\\u25E6|\\u2023|\\u25CF', '-', text)  # Replace various bullet characters with '-'\n",
        "    text = re.sub(r'\\*+', '', text)  # Remove asterisks used for obfuscation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Turn all whitespace into single spaces, including newlines\n",
        "    text = re.sub(r'\\s*([()])\\s*', r'\\1', text)\n",
        "    return text.strip()\n",
        "\n",
        "# df['text'] = df['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "sqv-i-uphMEz",
        "outputId": "dc7bb76d-ca27-45e4-eb76-0cb61450a48d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-fa1d01ae-107b-441a-bfc3-bbdd27c9f489\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>Word Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>python courses python courses, python exercise...</td>\n",
              "      <td>academic interests</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the learning point open digital education. a r...</td>\n",
              "      <td>academic interests</td>\n",
              "      <td>147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tech news, latest technology, mobiles, laptops...</td>\n",
              "      <td>academic interests</td>\n",
              "      <td>143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the best it certification materials in usa | k...</td>\n",
              "      <td>academic interests</td>\n",
              "      <td>364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bioland scientific, for your research needs bi...</td>\n",
              "      <td>academic interests</td>\n",
              "      <td>176</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa1d01ae-107b-441a-bfc3-bbdd27c9f489')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fa1d01ae-107b-441a-bfc3-bbdd27c9f489 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fa1d01ae-107b-441a-bfc3-bbdd27c9f489');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2bc536a6-d45c-4796-a76d-efb44a938764\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2bc536a6-d45c-4796-a76d-efb44a938764')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2bc536a6-d45c-4796-a76d-efb44a938764 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                text              target  \\\n",
              "0  python courses python courses, python exercise...  academic interests   \n",
              "1  the learning point open digital education. a r...  academic interests   \n",
              "2  tech news, latest technology, mobiles, laptops...  academic interests   \n",
              "3  the best it certification materials in usa | k...  academic interests   \n",
              "4  bioland scientific, for your research needs bi...  academic interests   \n",
              "\n",
              "   Word Count  \n",
              "0         125  \n",
              "1         147  \n",
              "2         143  \n",
              "3         364  \n",
              "4         176  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8-wAEJPpQ8k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class DialogueTemplate():\n",
        "    # system_token = \"<|system|>\" #system prompt\n",
        "    user_token = \"<|user|>\"\n",
        "    assistant_token = \"<|assistant|>\"\n",
        "    # end_token = \"<|end|>\"\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    def get_special_tokens(self):\n",
        "        return [self.user_token, self.assistant_token]\n",
        "\n",
        "    def get_training_prompt(self, infer = False):\n",
        "        prompt = self.user_token + clean_text(self.messages['text']) + '\\n' + self.assistant_token + self.messages['target']\n",
        "        if infer == False:\n",
        "          prompt += '<|endoftext|>'\n",
        "        # prompt = prompt +  ' <|endoftext|>'\n",
        "        return prompt #fully formed tranining prompt\n",
        "\n",
        "    def get_inference_prompt(self, text_c):\n",
        "        prompt = self.user_token + text_c + '\\n' + self.assistant_token\n",
        "        return prompt\n",
        "\n",
        "    def get_raw_dialogue(self):\n",
        "        prompt = ''\n",
        "        for message in self.messages:\n",
        "            prompt += message[\"content\"] + '\\n'\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def prepare_dialogue(self, example):\n",
        "        self.messages = example# for rpg quest\n",
        "        # print(\"Message is:\", self.messages)\n",
        "\n",
        "        # example['text'] = self.get_training_prompt()\n",
        "        return self.get_training_prompt()\n",
        "\n",
        "def get_dialogue_template():\n",
        "    return DialogueTemplate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je9d0VJJKQ9l",
        "outputId": "06ee7980-aaba-4d97-8af8-ceb6fcc88a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFLudWGqb7DI"
      },
      "outputs": [],
      "source": [
        "from pickle import FALSE\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from random import shuffle\n",
        "\n",
        "# from dialogue import get_dialogue_template\n",
        "\n",
        "class  DialogueDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_samples, seq_length=1024):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_length = seq_length\n",
        "        self.dialogue_template = get_dialogue_template()\n",
        "\n",
        "        self.shuffle()\n",
        "        print(\"DATA LENGTH: \", len(data))\n",
        "        if max_samples > 0:\n",
        "            self.data = self.data[:max_samples]\n",
        "\n",
        "        # dialogue_tokens = self.dialogue_template.get_special_tokens()\n",
        "        # print(\"dialogue tokens are: \",dialogue_tokens)\n",
        "        # self.tokenizer.add_special_tokens({\"additional_special_tokens\": dialogue_tokens})\n",
        "        # self.tokenizer.add_special_tokens({'pad_token':'<pad>'})\n",
        "\n",
        "    def shuffle(self):\n",
        "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
        "        return self.data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx]\n",
        "        diag = self.dialogue_template.prepare_dialogue(sample)\n",
        "\n",
        "        input = self.tokenizer(diag, return_token_type_ids=False,max_length=self.seq_length,truncation=True,padding='max_length',return_tensors='pt')\n",
        "\n",
        "        input['input_ids'] = input['input_ids'].squeeze(0)\n",
        "        input['attention_mask'] = input['attention_mask'].squeeze(0)\n",
        "        # labels = torch.empty_like(input['input_ids']).copy_(input[\"input_ids\"])\n",
        "        labels = torch.empty_like(input['input_ids']).copy_(input[\"input_ids\"])\n",
        "\n",
        "        labels = self.mask_user_labels(labels)\n",
        "\n",
        "        # labels = torch.cat([labels[1:], torch.full((1,), self.tokenizer.encode('<pad>')[0],dtype=labels.dtype)],dim=0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\" : input[\"input_ids\"],\n",
        "            \"attention_mask\" :input[\"attention_mask\"],\n",
        "            \"labels\":labels,\n",
        "        }\n",
        "    def mask_user_labels(self, labels):\n",
        "        user_token_id = self.tokenizer.convert_tokens_to_ids(self.dialogue_template.user_token)\n",
        "        assistant_token_id = self.tokenizer.convert_tokens_to_ids(self.dialogue_template.assistant_token)\n",
        "        # system_token_id = self.tokenizer.convert_tokens_to_ids(self.dialogue_template.system_token)\n",
        "        pad_token_id = self.tokenizer.convert_tokens_to_ids('<|pad|>')\n",
        "        masking = False\n",
        "        for idx, label_id in enumerate(labels):\n",
        "            marked = False\n",
        "            if label_id in [user_token_id]:\n",
        "                masking = True  # Start masking when user token is encountered\n",
        "            if label_id == assistant_token_id:\n",
        "                labels[idx] = pad_token_id  # Ensure assistant token is masked\n",
        "                masking = False\n",
        "            if masking:\n",
        "                labels[idx] = pad_token_id  # Mask current token if in masking mode\n",
        "\n",
        "        return labels\n",
        "\n",
        "\n",
        "def get_dialogue_dataset(data, tokenizer, max_samples,seq_length):\n",
        "    return DialogueDataset(data, tokenizer, max_samples, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1F1JvwN2OIC",
        "outputId": "7b00c40a-df98-4532-d7b6-91ebc4fcf684"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA LENGTH:  697527\n",
            "input_ids:  <|user|>in the same corner. I sat on it for the first time just started crying saying, i12i12i12Ya Allah please forgive my sins.i12i12i12 That was 2012. I started reading the Quran and the translations and it connected with me really fast. I started practising Islam and learnt how to pray and by January 2014, felt sure about converting. Since I am known as Yuvan Shankar Raja in films, I have still not changed my name officially in my passport and other records, but maybe later, I might do that too. My father was the last one I told in my house to. I told him, i12i12i12I have started reading the Quran and it gives me a lot of peace.i12i12i12 He said to me, i12i12i12Yuvan, I am not comfortable with you becoming Islamic.i12i12i12 My brother and his wife were very supportive. Iti12i12i12s odd but in some way I used to get that spiritual feeling that it was my mom, who held my hand, and said, i12i12i12Yuvan, you are alone. I want you to stand here under the tree called Islam.i12i12i12\n",
            "<|assistant|>family and relationships<|endoftext|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
            "labels:  <|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>family and relationships<|endoftext|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
            "attention_mask:  tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "Special:  ['<|user|>', '<|assistant|>']\n",
            "vocab size:  50260  ids:  [50257, 50258]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 500/697527 [00:00<17:38, 658.35it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_from_file(filename):\n",
        "    with open(filename, 'r', encoding='utf-8',errors=\"ignore\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def decode_masked_labels(labels, tokenizer):\n",
        "    # Replace -100 with tokenizer.pad_token_id\n",
        "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    labels = torch.where(labels != -100, labels, torch.tensor(pad_id))\n",
        "    # Decode the labels\n",
        "    return tokenizer.decode(labels)\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "train_batch_size = 16\n",
        "test_batch_size = 4\n",
        "\n",
        "def get_ds(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    # if tokenizer.pad_token is None:\n",
        "    #     tokenizer.pad_token = tokenizer.eos_token\n",
        "    dialogue_template = get_dialogue_template()\n",
        "    special_tokens = dialogue_template.get_special_tokens()\n",
        "\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
        "    # raw_dataset = load_from_file(data_filename)\n",
        "    dataset = get_dialogue_dataset(df, tokenizer, 800000, 1024)\n",
        "\n",
        "\n",
        "    val_size = int(0.1 * len(dataset))\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, pin_memory=False)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer\n",
        "\n",
        "from collections import Counter\n",
        "def analyze_invalid_tokens(dataloader, tokenizer):\n",
        "    invalid_token_counter = Counter()\n",
        "    total_tokens = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Analyzing invalid tokens\"):\n",
        "        labels = batch[\"labels\"]\n",
        "        invalid_ids = (labels < 0) | (labels >= len(tokenizer))\n",
        "        invalid_token_counter.update(labels[invalid_ids].tolist())\n",
        "        total_tokens += labels.numel()\n",
        "\n",
        "    print(f\"Total tokens: {total_tokens}\")\n",
        "    # print(f\"Total invalid tokens: {sum(invalid_token_counter.values())}\")\n",
        "    print(f\"Unique invalid tokens: {len(invalid_token_counter)}\")\n",
        "    print(\"Top 10 invalid tokens:\")\n",
        "    for token, count in invalid_token_counter.most_common(10):\n",
        "        token_str = tokenizer.decode([token]) if token >= len(tokenizer) else \"\"\n",
        "        print(f\"Token ID {token}: {token_str} - {count} occurrences\")\n",
        "\n",
        "    return invalid_token_counter\n",
        "# data_filename = r\"G:\\Coding\\gpt2\\datasets\\internet_archive_scifi_v3.txt\"\n",
        "# data_filename = r\"/content/gdrive/MyDrive/GPT2-Finetuning/finetune_chatbot/finetune_chatbot/data/oasst.json\"\n",
        "# data_filename = data_file\n",
        "train_dataloader, val_dataloader, tokenizer = get_ds(model_name)\n",
        "for batch in train_dataloader:\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "\n",
        "    labels=batch[\"labels\"]\n",
        "\n",
        "    print(\"input_ids: \", tokenizer.decode(input_ids[0]))\n",
        "    print(\"labels: \", decode_masked_labels(labels[0], tokenizer))\n",
        "    print(\"attention_mask: \", attention_mask[0])\n",
        "\n",
        "    break\n",
        "# analyze_invalid_tokens(val_dataloader, tokenizer)\n",
        "\n",
        "print(\"Special: \" ,tokenizer.additional_special_tokens)\n",
        "special_token_ids = tokenizer.convert_tokens_to_ids(tokenizer.additional_special_tokens)\n",
        "print(\"vocab size: \", len(tokenizer.vocab), \" ids: \", special_token_ids)\n",
        "tokenized_size = []\n",
        "lines = 0\n",
        "sum =  0\n",
        "for id,row in tqdm(df.iterrows(), total=len(df)):\n",
        "    tokenized_input = tokenizer.tokenize(row['text'])\n",
        "    tokenized_size.append(len(tokenized_input))\n",
        "    lines += 1\n",
        "    if lines > 500:\n",
        "        break\n",
        "# print(tokenized_size)\n",
        "# np.histogram(tokenized_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "3gGd1FT3pO3E",
        "outputId": "cd74e0ad-2f9a-479d-9cde-7a8773bc6889"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2: 100%|██████████| 39236/39236 [6:33:32<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average train loss: 0.2344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 17438/17438 [33:11<00:00,  8.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.1190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 39236/39236 [6:33:37<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average train loss: 0.1040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 17438/17438 [32:20<00:00,  8.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.1118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "def finetune_gpt2(train_dataloader, val_dataloader, tokenizer, model, epochs=2, learning_rate=5e-5, warmup_steps=1000, max_grad_norm=1.0, device=\"cuda\"):\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            labels[labels == pad_token_id] = -100\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Average train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                labels[labels == pad_token_id] = -100\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
        "        model.save_pretrained(\"/content/drive/MyDrive/fibe-1024-distilgpt2-full\")\n",
        "        tokenizer.save_pretrained(\"/content/drive/MyDrive/fibe-1024-distilgpt2-full\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "model, tokenizer = finetune_gpt2(train_dataloader, val_dataloader, tokenizer,model, device=\"cuda\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/fibe-1024-distilgpt2-full\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/fibe-1024-distilgpt2-full\")\n",
        "from google.colab import runtime #disconnect after saving on long runs\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHmAI0qSs0ke"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "# modelname = r\"gpt2\"\n",
        "modelname = \"/content/drive/MyDrive/fibe-1024-distilgpt2-full\"\n",
        "model = AutoModelForCausalLM.from_pretrained(modelname).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelname,padding_side='left')\n",
        "# tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CteJyWlfADyl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "df = pd.read_csv('/content/dataset/test.csv',encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogLAuNzdElzo"
      },
      "outputs": [],
      "source": [
        "def batched_inference(model, tokenizer, prompts, max_new_tokens=128, temperature=0.3, top_k=100, top_p=0.95):\n",
        "    model.eval()\n",
        "    dialogue_template = get_dialogue_template()\n",
        "    device = \"cuda\"\n",
        "\n",
        "    # Prepare batched input prompts\n",
        "    input_texts = [dialogue_template.get_inference_prompt(prompt) for prompt in prompts]\n",
        "\n",
        "    # Tokenize all prompts in a batch\n",
        "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # Generate output for the whole batch\n",
        "      outputs = model.generate(\n",
        "          input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          max_new_tokens=max_new_tokens,\n",
        "          temperature=temperature,\n",
        "          top_k=top_k,\n",
        "          top_p=top_p,\n",
        "          do_sample=True,\n",
        "          pad_token_id=tokenizer.pad_token_id,\n",
        "      )\n",
        "\n",
        "    # Decode each output in the batch and clean up the text\n",
        "    responses = []\n",
        "    for output in outputs:\n",
        "        generated_text = tokenizer.decode(output, skip_special_tokens=False)\n",
        "        response = generated_text.split(dialogue_template.assistant_token)[1].strip()\n",
        "        response = response.replace(\"<|endoftext|>\", \"\").strip()\n",
        "        responses.append(response)\n",
        "\n",
        "    return responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6RoDvTqEqwD",
        "outputId": "00b00235-c7d1-4004-c629-06c75a979cd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 187/10899 [01:02<55:03,  3.24it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 10899/10899 [1:14:22<00:00,  2.44it/s]\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16  # Define the batch size\n",
        "out_df = pd.DataFrame(columns=[\"target\", \"Index\"])\n",
        "\n",
        "# Loop through the dataframe in batches\n",
        "for i in tqdm(range(0, len(df), batch_size)):\n",
        "    batch = df.iloc[i:i+batch_size]  # Get a batch of rows\n",
        "    lst = batch['text'].tolist()  # Extract the 'text' column (input prompts)\n",
        "    prompts = []\n",
        "    for txt in lst:\n",
        "      cleaned = clean_text(txt)\n",
        "      if len(tokenizer.tokenize(cleaned)) > 1000:\n",
        "        encoded_tokens = tokenizer.encode(txt, truncation=True, max_length=1000)\n",
        "        # Decode the tokens back to text (optional)\n",
        "        cleaned = tokenizer.decode(encoded_tokens, skip_special_tokens=True)\n",
        "\n",
        "      prompts.append(cleaned)\n",
        "\n",
        "    # Get batched responses\n",
        "    try:\n",
        "      responses = batched_inference(model, tokenizer, prompts, top_k=1, temperature=0.01, top_p=1.0, max_new_tokens=64)\n",
        "    except:\n",
        "      print(\"Error on: \", i)\n",
        "      break\n",
        "\n",
        "    # Store the responses in out_df along with the 'Index'\n",
        "    for j, response in enumerate(responses):\n",
        "        out_df.loc[batch.index[j]] = [response, batch.iloc[j]['Index']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "PM82MsT54Vbx",
        "outputId": "97e0d30e-dfec-46b9-ba6a-bf98839e9b01"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'out_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-49db90dc422e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/fibe_out_all_1024.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'out_df' is not defined"
          ]
        }
      ],
      "source": [
        "out_df.to_csv('/content/drive/MyDrive/fibe_out_all_1024.csv')\n",
        "#requires some postprocessing to remove padding tokens and adding quotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YGLsk5SCwm8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
